{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys, os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pickle as pkl\n",
    "\n",
    "sys.path.append(os.path.abspath('../util'))\n",
    "# sys.path.append(os.path.abspath('../'))\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('../data/')\n",
    "model_path = Path('../model/')\n",
    "# data_genre = data_path / Path('data_w_genres.csv')\n",
    "df_path= data_path / Path('df_cleaned__by_artist.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV file contains float columns as features and using artist as a label for multiclass prediction.\n",
    "If a song was produced by >1 artist the row has been split into mulitple rows so that each row only\n",
    "has one artist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>energy</th>\n",
       "      <th>explicit</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>key</th>\n",
       "      <th>liveness</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>popularity</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>valence</th>\n",
       "      <th>artists</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.998996</td>\n",
       "      <td>0.716599</td>\n",
       "      <td>0.028442</td>\n",
       "      <td>0.1950</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.1510</td>\n",
       "      <td>0.745000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052219</td>\n",
       "      <td>0.485348</td>\n",
       "      <td>0.7790</td>\n",
       "      <td>Carl Woitschach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.997992</td>\n",
       "      <td>0.383603</td>\n",
       "      <td>0.051316</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.0763</td>\n",
       "      <td>0.494026</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047678</td>\n",
       "      <td>0.344019</td>\n",
       "      <td>0.0767</td>\n",
       "      <td>Robert Schumann</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.997992</td>\n",
       "      <td>0.383603</td>\n",
       "      <td>0.051316</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.0763</td>\n",
       "      <td>0.494026</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047678</td>\n",
       "      <td>0.344019</td>\n",
       "      <td>0.0767</td>\n",
       "      <td>Vladimir Horowitz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.606426</td>\n",
       "      <td>0.758097</td>\n",
       "      <td>0.018374</td>\n",
       "      <td>0.2200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.1190</td>\n",
       "      <td>0.627609</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.958720</td>\n",
       "      <td>0.439086</td>\n",
       "      <td>0.8800</td>\n",
       "      <td>Seweryn Goszczyński</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.998996</td>\n",
       "      <td>0.790486</td>\n",
       "      <td>0.032538</td>\n",
       "      <td>0.1300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.887</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.1110</td>\n",
       "      <td>0.708887</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.095562</td>\n",
       "      <td>0.442470</td>\n",
       "      <td>0.7200</td>\n",
       "      <td>Francisco Canaro</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   acousticness  danceability  duration_ms  energy  explicit  \\\n",
       "0      0.998996      0.716599     0.028442  0.1950       0.0   \n",
       "1      0.997992      0.383603     0.051316  0.0135       0.0   \n",
       "2      0.997992      0.383603     0.051316  0.0135       0.0   \n",
       "3      0.606426      0.758097     0.018374  0.2200       0.0   \n",
       "4      0.998996      0.790486     0.032538  0.1300       0.0   \n",
       "\n",
       "   instrumentalness       key  liveness  loudness  mode  popularity  \\\n",
       "0             0.563  0.909091    0.1510  0.745000   1.0         0.0   \n",
       "1             0.901  0.727273    0.0763  0.494026   1.0         0.0   \n",
       "2             0.901  0.727273    0.0763  0.494026   1.0         0.0   \n",
       "3             0.000  0.454545    0.1190  0.627609   0.0         0.0   \n",
       "4             0.887  0.090909    0.1110  0.708887   0.0         0.0   \n",
       "\n",
       "   speechiness     tempo  valence              artists  \n",
       "0     0.052219  0.485348   0.7790      Carl Woitschach  \n",
       "1     0.047678  0.344019   0.0767      Robert Schumann  \n",
       "2     0.047678  0.344019   0.0767    Vladimir Horowitz  \n",
       "3     0.958720  0.439086   0.8800  Seweryn Goszczyński  \n",
       "4     0.095562  0.442470   0.7200     Francisco Canaro  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(df_path,index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists_names = df.artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_artist = dict(((i, artist) for i,artist in  enumerate(artists_names.unique())))\n",
    "artist_index = dict((val, key) for key,val in index_artist.items() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['artists'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Queen', 2176)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_artist[2176] , artist_index['Queen']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "- Create a generator with True/ Fake samples\n",
    "    - Fake samples: Choose a random vector and assign a label that is not its true artist label (1/-1)\n",
    "- Create an embedding layer for the artists and pass the remaining features to a FC layer\n",
    "- Train the model to see if it can detect fakes (BCE)\n",
    "- User inputs song\n",
    "    - Featurize the song using spotify api\n",
    "    - Input the features into model(Artist in dict | Artist not in dict)\n",
    "    - Works for Artist in dict, if artist not in dict???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Paul McCartney', 2032, 'Paul McCartney')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artists_names[5000], artist_index[artists_names[5000]], index_artist[2032]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, df, cols, is_truth = 1):\n",
    "        'Initialization'\n",
    "        self.df = df\n",
    "#         self.false_pct = false_pct\n",
    "        self.cols = cols\n",
    "        self.is_truth = is_truth\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        true_label = 1\n",
    "        fake_label = -1\n",
    "        \n",
    "        if self.is_truth: \n",
    "            return torch.tensor([self.df[self.cols].iloc[index]]).double(), \\\n",
    "                   artist_index[artists_names[index]],\\\n",
    "                   torch.tensor([true_label])\n",
    "        else:\n",
    "            return torch.rand(len(list(df.columns))).unsqueeze(0).double(), \\\n",
    "                   artist_index[artists_names[random.choice(range(0,len(df)))]], \\\n",
    "                   torch.tensor([fake_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepEmbed(nn.Module):\n",
    "\n",
    "    def __init__(self, feature_size, lookup_size, embedding_dim, batch_size):\n",
    "        super(DeepEmbed, self).__init__()\n",
    "        # Should Input size be (None,1) ?         \n",
    "        self.embeddings = nn.Embedding(lookup_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
    "        self.head = nn.Linear(feature_size, 128)\n",
    "        self.op_layer = nn.Linear(256, 1)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def forward(self, feature_input, lookup_input):\n",
    "        embeds = self.embeddings(lookup_input).view(self.batch_size, 1, -1)\n",
    "        out1 = F.relu(self.linear1(embeds))\n",
    "        out2 = F.relu(self.head(feature_input))\n",
    "        output_concat = torch.cat([out1, out2], axis =-1)\n",
    "        out = torch.sigmoid(self.op_layer(output_concat))\n",
    "        return out.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_set = Dataset(df, list(df.columns), is_truth=True)\n",
    "fake_set = Dataset(df, list(df.columns), is_truth=False)\n",
    "full_ds = torch.utils.data.ConcatDataset([truth_set, fake_set])\n",
    "# training_generator = torch.utils.data.DataLoader(training_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_size = len(artists_names.unique())\n",
    "emb_dim = 50\n",
    "feature_size = len(df.columns)\n",
    "learning_rate = 1e-4\n",
    "batch_size = 256\n",
    "epochs = 200 \n",
    "\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = torch.utils.data.DataLoader(full_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "_iter = iter(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 1, 14])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features, artist, label = next(_iter)\n",
    "# (features.shape), artist, index_artist[int(artist)] , label\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepEmbed(\n",
      "  (embeddings): Embedding(31465, 50)\n",
      "  (linear1): Linear(in_features=50, out_features=128, bias=True)\n",
      "  (head): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (op_layer): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = DeepEmbed(feature_size = feature_size, lookup_size=lookup_size, \\\n",
    "                  embedding_dim=emb_dim, batch_size=batch_size)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for epoch in range(epochs):\n",
    "        if epoch %25 == 0 :\n",
    "            print(f\"--EPOCH {epoch}--\")\n",
    "        for i in train_loader:\n",
    "            feature, artist, label = i\n",
    "            feature, artist, label = feature.cuda(), artist.cuda(),label.cuda()\n",
    "            feature = feature.float()\n",
    "            label = label.float()\n",
    "            output = model(feature, artist)\n",
    "            output = output\n",
    "\n",
    "            loss = criterion(output, label) \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if epoch %25 == 0 :\n",
    "                print(f\"{loss.item()}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if train_model == True:\n",
    "    train(model.cuda(), device, train_dl, optimizer, criterion)\n",
    "else:\n",
    "    model.load_state_dict(torch.load(model_path/'first_model.ckpt' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(),model_path/'first_model.ckpt' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = model.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4301, -0.1413, -0.5302,  1.2199,  2.9091,  1.9989, -0.4521, -0.1614,\n",
       "         -0.2575,  1.3220,  0.6549,  0.4241, -0.4755,  1.3491,  0.4467, -0.5560,\n",
       "          0.1040, -1.0075,  0.7730,  0.9286, -0.6500,  0.2600, -1.2071,  1.0382,\n",
       "          0.6945, -0.5230, -1.6711, -0.5926,  1.6206, -0.6386,  1.1328, -0.5365,\n",
       "          0.4495,  1.6935, -0.4167, -0.6155,  2.1285,  1.1144, -0.6684,  1.1695,\n",
       "         -0.5057,  2.2098, -0.1938,  0.8068, -1.2424,  1.1053,  1.5306, -0.2038,\n",
       "          0.1986,  0.9018]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb(torch.tensor([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict() \n",
    "for i in range(1, len(artist_index)):\n",
    "    d[index_artist[i]] = emb(torch.tensor([i])).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_path/ 'deep_embeddings.pkl', 'wb') as handle:\n",
    "    pkl.dump(d, handle, protocol=pkl.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
